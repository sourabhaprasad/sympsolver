{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers\n",
    "\n",
    "# Pinecone API credentials and index details\n",
    "PINECONE_API_KEY = \"2bd5fd9f-2c56-42f9-aa1b-037960262fff\"\n",
    "PINECONE_API_ENV = \"us-east-1-aws\"\n",
    "INDEX_NAME = \"medicalbot\"\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_API_ENV)\n",
    "\n",
    "# Load data from PDF files\n",
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(\n",
    "        data,\n",
    "        glob=\"*.pdf\",\n",
    "        loader_cls=PyPDFLoader\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "extracted_data = load_pdf(\"data/\")\n",
    "print(f\"Loaded {len(extracted_data)} documents from PDF files.\")\n",
    "\n",
    "# Split text into manageable chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=20\n",
    "    )\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks\n",
    "\n",
    "text_chunks = text_split(extracted_data)\n",
    "print(\"Length of text chunks:\", len(text_chunks))\n",
    "\n",
    "# Download Hugging Face embeddings\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings\n",
    "\n",
    "embeddings = download_hugging_face_embeddings()\n",
    "\n",
    "# Connect to existing Pinecone index and add new data if needed\n",
    "try:\n",
    "    docsearch = Pinecone.from_existing_index(index_name=INDEX_NAME, embedding=embeddings)\n",
    "    print(\"Connected to existing Pinecone index.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to existing index: {e}\")\n",
    "    print(\"Creating a new index and storing data...\")\n",
    "    docsearch = Pinecone.from_texts(\n",
    "        [t.page_content for t in text_chunks], \n",
    "        embeddings, \n",
    "        index_name=INDEX_NAME\n",
    "    )\n",
    "\n",
    "# Define query for similarity search\n",
    "query = \"What are Allergies?\"\n",
    "docs = docsearch.similarity_search(query, k=3)\n",
    "print(\"Retrieved documents:\", docs)\n",
    "\n",
    "# Define the PromptTemplate for the chatbot\n",
    "prompt_template = \"\"\"\n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know; don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = CTransformers(\n",
    "    model=\"model/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "    model_type=\"llama\",\n",
    "    config={\n",
    "        'max_new_tokens': 512,\n",
    "        'temperature': 0.8\n",
    "    }\n",
    ")\n",
    "\n",
    "# Build the RetrievalQA chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever(search_kwargs={'k': 2}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs=chain_type_kwargs\n",
    ")\n",
    "\n",
    "# Query the QA system\n",
    "result = qa.run(query)\n",
    "print(\"Answer:\", result)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
